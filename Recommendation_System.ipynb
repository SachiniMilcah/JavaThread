{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommendation_System.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMtmOeS8PdVPR7HztWydnsR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MflPHcxiu9fw"
      },
      "source": [
        "!pip install -U -q PyDrive\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "# Authenticate and create the PyDrive client.\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFBQ6As3wu2g"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1x4oWOgcpSWBtc6R5N5L6JxJInXNxstCI'})\r\n",
        "downloaded.GetContentFile('OnlineRetail.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "_9Nl_8qw15VJ",
        "outputId": "7298e7af-63b8-4d06-9c8c-c84525cb7651"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import random\r\n",
        "data = pd.read_csv('OnlineRetail.csv' ,encoding= 'unicode_escape',nrows=20000)\r\n",
        "pd.set_option('display.max_columns', None)\r\n",
        "data = data.dropna()\r\n",
        "data = data.reset_index()\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>InvoiceNo</th>\n",
              "      <th>StockCode</th>\n",
              "      <th>Description</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>InvoiceDate</th>\n",
              "      <th>UnitPrice</th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>536365</td>\n",
              "      <td>85123A</td>\n",
              "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>6</td>\n",
              "      <td>12/1/2010 8:26</td>\n",
              "      <td>2.55</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>536365</td>\n",
              "      <td>71053</td>\n",
              "      <td>WHITE METAL LANTERN</td>\n",
              "      <td>6</td>\n",
              "      <td>12/1/2010 8:26</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>536365</td>\n",
              "      <td>84406B</td>\n",
              "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
              "      <td>8</td>\n",
              "      <td>12/1/2010 8:26</td>\n",
              "      <td>2.75</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>536365</td>\n",
              "      <td>84029G</td>\n",
              "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
              "      <td>6</td>\n",
              "      <td>12/1/2010 8:26</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>536365</td>\n",
              "      <td>84029E</td>\n",
              "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
              "      <td>6</td>\n",
              "      <td>12/1/2010 8:26</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index InvoiceNo StockCode                          Description  Quantity  \\\n",
              "0      0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
              "1      1    536365     71053                  WHITE METAL LANTERN         6   \n",
              "2      2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
              "3      3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
              "4      4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
              "\n",
              "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
              "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
              "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
              "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
              "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
              "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "sk7Dvv_M2qhG",
        "outputId": "0ce5627d-7d8d-481a-9ee6-b8cff8348346"
      },
      "source": [
        "data = data.drop(['InvoiceNo','InvoiceDate','UnitPrice','Country'], axis=1)\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>StockCode</th>\n",
              "      <th>Description</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>CustomerID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>85123A</td>\n",
              "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>71053</td>\n",
              "      <td>WHITE METAL LANTERN</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>84406B</td>\n",
              "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
              "      <td>8</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>84029G</td>\n",
              "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>84029E</td>\n",
              "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index StockCode                          Description  Quantity  CustomerID\n",
              "0      0    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6     17850.0\n",
              "1      1     71053                  WHITE METAL LANTERN         6     17850.0\n",
              "2      2    84406B       CREAM CUPID HEARTS COAT HANGER         8     17850.0\n",
              "3      3    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6     17850.0\n",
              "4      4    84029E       RED WOOLLY HOTTIE WHITE HEART.         6     17850.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzNOjRUX2vC6"
      },
      "source": [
        "#content-based recommendations\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "vector_result = vectorizer.fit_transform(data['Description'].values)\r\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "laPSSCNQ3C-W",
        "outputId": "aaf7dfc5-ed71-4ad8-d930-ee9f866c5c42"
      },
      "source": [
        "vect_description = pd.DataFrame(vector_result.toarray(), columns=feature_names)\r\n",
        "vect_description['wordbag']= vect_description.values.tolist()\r\n",
        "data['Description'] = vect_description['wordbag']\r\n",
        "data.head()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>StockCode</th>\n",
              "      <th>Description</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>CustomerID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>85123A</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>71053</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>84406B</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>8</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>84029G</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>84029E</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>6</td>\n",
              "      <td>17850.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index StockCode                                        Description  \\\n",
              "0      0    85123A  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "1      1     71053  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2      2    84406B  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "3      3    84029G  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "4      4    84029E  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "\n",
              "   Quantity  CustomerID  \n",
              "0         6     17850.0  \n",
              "1         6     17850.0  \n",
              "2         8     17850.0  \n",
              "3         6     17850.0  \n",
              "4         6     17850.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vge0HWJ3Hmw"
      },
      "source": [
        "def get_similar_products(Stockid, top):\r\n",
        "    products = data[['StockCode', 'Description']]\r\n",
        "    products  = products [products .StockCode != Stockid]\r\n",
        "    products ['distance'] = products ['Description'].apply(lambda x: cosine_similarity(np.array(x).reshape(1, -1), np.array(data.loc[data['StockCode'] == Stockid]['Description'].values[0]).reshape(1, -1)))\r\n",
        "    products  = products.drop(columns=['Description'])\r\n",
        "    products  = products .explode('distance').explode('distance')\r\n",
        "    return products.sort_values(by=['distance'], ascending=False)['StockCode'].head(top).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x1sRf8u3M_r"
      },
      "source": [
        "def get_highly_purchased(userid):\r\n",
        "    purchase = data[data.CustomerID == user_id]\r\n",
        "    purchase = purchase[purchase['Quantity']> 30]\r\n",
        "    top_purchase = (purchase.sort_values(by=\"Quantity\", ascending=False).head(20))\r\n",
        "    top_purchase['purchased'] = top_purchase['StockCode']\r\n",
        "    top_purchase = top_purchase[['CustomerID', 'purchased']]\r\n",
        "    top_purchase['similar'] = top_purchase['purchased'].apply(lambda x: (get_similar_products(x, 5)))\r\n",
        "    result = [x for x in np.concatenate(top_purchase['similar'].values, axis=0).tolist() if x not in top_purchase.purchased.values.tolist()]\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIpPqY-J3QY6"
      },
      "source": [
        "def get_recomended(id, top):\r\n",
        "    similar_products = get_highly_purchased(id)\r\n",
        "    mean_quantity = pd.DataFrame(data.groupby('StockCode')['Quantity'].mean())\r\n",
        "    mean_quantity['rating_counts'] = pd.DataFrame(data.groupby('StockCode')['Quantity'].count())\r\n",
        "    mean_quantity = mean_quantity[mean_quantity['rating_counts'] > 50]\r\n",
        "    return mean_quantity[mean_quantity.index.isin(similar_products)].sort_values(by=['Quantity'], ascending=False).head(top)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVGvSPS03T6E"
      },
      "source": [
        "customer_id = data[\"CustomerID\"].unique().tolist()\r\n",
        "encode_user = {x: i for i, x in enumerate(customer_id)}\r\n",
        "encoded_userid = {i: x for i, x in enumerate(customer_id)}\r\n",
        "\r\n",
        "stock_code = data[\"StockCode\"].unique().tolist()\r\n",
        "encode_stock = {x: i for i, x in enumerate(stock_code)}\r\n",
        "encoded_stockid = {i: x for i, x in enumerate(stock_code)}\r\n",
        "\r\n",
        "data[\"Customer_ID\"] = data[\"CustomerID\"].map(encode_user)\r\n",
        "data[\"Stock_Code\"] = data[\"StockCode\"].map(encode_stock)\r\n",
        "\r\n",
        "num_users = len(encode_user)\r\n",
        "num_items = len(encoded_stockid)\r\n",
        "data[\"Qty\"] = data[\"Quantity\"].values.astype(np.float32)\r\n",
        "\r\n",
        "min_rating = min(data[\"Quantity\"])\r\n",
        "max_rating = max(data[\"Quantity\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zApuv1we3Xsz"
      },
      "source": [
        "df = data.sample(frac=1, random_state=42)\r\n",
        "\r\n",
        "x = df[[\"Customer_ID\", \"Stock_Code\"]].values\r\n",
        "y = df[\"Qty\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\r\n",
        "\r\n",
        "train_indices = int(0.9 * df.shape[0])\r\n",
        "x_train, x_val, y_train, y_val = (x[:train_indices],x[train_indices:],y[:train_indices],y[train_indices:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxWEzk6t3c4S"
      },
      "source": [
        "class Recommend(keras.Model):\r\n",
        "    def __init__(self, num_users, num_items, embedding_size, **kwargs):\r\n",
        "        super(Recommend, self).__init__(**kwargs)\r\n",
        "        self.num_users = num_users\r\n",
        "        self.num_items = num_items\r\n",
        "        self.embedding_size = embedding_size\r\n",
        "        self.user_embedding = layers.Embedding(num_users,embedding_size,embeddings_initializer=\"he_normal\",embeddings_regularizer=keras.regularizers.l2(1e-6),)\r\n",
        "        self.user_bias = layers.Embedding(num_users, 1)\r\n",
        "        self.item_embedding = layers.Embedding(num_items,embedding_size,embeddings_initializer=\"he_normal\", embeddings_regularizer=keras.regularizers.l2(1e-6),)\r\n",
        "        self.item_bias = layers.Embedding(num_items, 1)\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        user_vector = self.user_embedding(inputs[:, 0])\r\n",
        "        user_bias = self.user_bias(inputs[:, 0])\r\n",
        "        item_vector = self.item_embedding(inputs[:, 1])\r\n",
        "        item_bias = self.item_bias(inputs[:, 1])\r\n",
        "        items = tf.tensordot(user_vector, item_vector, 2)\r\n",
        "        x = items + user_bias + item_bias\r\n",
        "        return tf.nn.sigmoid(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUmjXF8c3gMp",
        "outputId": "f57985cf-f6df-4f0f-85a0-ad6c5ca3dc71"
      },
      "source": [
        "model = Recommend(num_users, num_items, 50)\r\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=0.0005))\r\n",
        "history = model.fit(x=x_train,y=y_train, batch_size=20, epochs=250, verbose=1, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.6850 - val_loss: 0.6449\n",
            "Epoch 2/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5869 - val_loss: 0.6027\n",
            "Epoch 3/250\n",
            "601/601 [==============================] - 2s 2ms/step - loss: 0.5534 - val_loss: 0.5896\n",
            "Epoch 4/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5477 - val_loss: 0.5893\n",
            "Epoch 5/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5465 - val_loss: 0.5881\n",
            "Epoch 6/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5463 - val_loss: 0.5867\n",
            "Epoch 7/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5460 - val_loss: 0.5870\n",
            "Epoch 8/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5459 - val_loss: 0.5849\n",
            "Epoch 9/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5458 - val_loss: 0.5825\n",
            "Epoch 10/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5461 - val_loss: 0.5815\n",
            "Epoch 11/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5458 - val_loss: 0.5807\n",
            "Epoch 12/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5459 - val_loss: 0.5806\n",
            "Epoch 13/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5462 - val_loss: 0.5777\n",
            "Epoch 14/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5462 - val_loss: 0.5761\n",
            "Epoch 15/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5464 - val_loss: 0.5753\n",
            "Epoch 16/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5466 - val_loss: 0.5741\n",
            "Epoch 17/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5462 - val_loss: 0.5727\n",
            "Epoch 18/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5467 - val_loss: 0.5714\n",
            "Epoch 19/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5464 - val_loss: 0.5677\n",
            "Epoch 20/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5466 - val_loss: 0.5680\n",
            "Epoch 21/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5466 - val_loss: 0.5683\n",
            "Epoch 22/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5464 - val_loss: 0.5670\n",
            "Epoch 23/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5465 - val_loss: 0.5645\n",
            "Epoch 24/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5463 - val_loss: 0.5635\n",
            "Epoch 25/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5461 - val_loss: 0.5628\n",
            "Epoch 26/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5460 - val_loss: 0.5624\n",
            "Epoch 27/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5460 - val_loss: 0.5622\n",
            "Epoch 28/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5462 - val_loss: 0.5611\n",
            "Epoch 29/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5461 - val_loss: 0.5612\n",
            "Epoch 30/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5458 - val_loss: 0.5613\n",
            "Epoch 31/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5460 - val_loss: 0.5587\n",
            "Epoch 32/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5458 - val_loss: 0.5585\n",
            "Epoch 33/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5461 - val_loss: 0.5584\n",
            "Epoch 34/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5458 - val_loss: 0.5566\n",
            "Epoch 35/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5460 - val_loss: 0.5577\n",
            "Epoch 36/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5457 - val_loss: 0.5579\n",
            "Epoch 37/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5457 - val_loss: 0.5571\n",
            "Epoch 38/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5458 - val_loss: 0.5561\n",
            "Epoch 39/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5458 - val_loss: 0.5562\n",
            "Epoch 40/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5455 - val_loss: 0.5542\n",
            "Epoch 41/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5458 - val_loss: 0.5546\n",
            "Epoch 42/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5455 - val_loss: 0.5533\n",
            "Epoch 43/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5457 - val_loss: 0.5545\n",
            "Epoch 44/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5457 - val_loss: 0.5537\n",
            "Epoch 45/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5456 - val_loss: 0.5525\n",
            "Epoch 46/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5456 - val_loss: 0.5510\n",
            "Epoch 47/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5455 - val_loss: 0.5515\n",
            "Epoch 48/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5455 - val_loss: 0.5520\n",
            "Epoch 49/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5457 - val_loss: 0.5506\n",
            "Epoch 50/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5454 - val_loss: 0.5505\n",
            "Epoch 51/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5454 - val_loss: 0.5500\n",
            "Epoch 52/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5454 - val_loss: 0.5504\n",
            "Epoch 53/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5454 - val_loss: 0.5495\n",
            "Epoch 54/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5455 - val_loss: 0.5497\n",
            "Epoch 55/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5455 - val_loss: 0.5494\n",
            "Epoch 56/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5454 - val_loss: 0.5492\n",
            "Epoch 57/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5488\n",
            "Epoch 58/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5486\n",
            "Epoch 59/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5489\n",
            "Epoch 60/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5489\n",
            "Epoch 61/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5454 - val_loss: 0.5491\n",
            "Epoch 62/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5492\n",
            "Epoch 63/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5486\n",
            "Epoch 64/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5482\n",
            "Epoch 65/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5484\n",
            "Epoch 66/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5484\n",
            "Epoch 67/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5482\n",
            "Epoch 68/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5479\n",
            "Epoch 69/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5477\n",
            "Epoch 70/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5478\n",
            "Epoch 71/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5475\n",
            "Epoch 72/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5474\n",
            "Epoch 73/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5453 - val_loss: 0.5473\n",
            "Epoch 74/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5470\n",
            "Epoch 75/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5469\n",
            "Epoch 76/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5469\n",
            "Epoch 77/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5467\n",
            "Epoch 78/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5471\n",
            "Epoch 79/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5469\n",
            "Epoch 80/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5467\n",
            "Epoch 81/250\n",
            "601/601 [==============================] - 2s 2ms/step - loss: 0.5452 - val_loss: 0.5469\n",
            "Epoch 82/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5469\n",
            "Epoch 83/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5467\n",
            "Epoch 84/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5468\n",
            "Epoch 85/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5466\n",
            "Epoch 86/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5466\n",
            "Epoch 87/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5452 - val_loss: 0.5470\n",
            "Epoch 88/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5464\n",
            "Epoch 89/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5466\n",
            "Epoch 90/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5465\n",
            "Epoch 91/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5466\n",
            "Epoch 92/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5462\n",
            "Epoch 93/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5469\n",
            "Epoch 94/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5468\n",
            "Epoch 95/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5462\n",
            "Epoch 96/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5463\n",
            "Epoch 97/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5461\n",
            "Epoch 98/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5463\n",
            "Epoch 99/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5460\n",
            "Epoch 100/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5461\n",
            "Epoch 101/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5464\n",
            "Epoch 102/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5463\n",
            "Epoch 103/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5452 - val_loss: 0.5465\n",
            "Epoch 104/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5462\n",
            "Epoch 105/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5461\n",
            "Epoch 106/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5451 - val_loss: 0.5463\n",
            "Epoch 107/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5459\n",
            "Epoch 108/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5459\n",
            "Epoch 109/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5461\n",
            "Epoch 110/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5460\n",
            "Epoch 111/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5462\n",
            "Epoch 112/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5458\n",
            "Epoch 113/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5460\n",
            "Epoch 114/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5458\n",
            "Epoch 115/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5460\n",
            "Epoch 116/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5458\n",
            "Epoch 117/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5456\n",
            "Epoch 118/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5458\n",
            "Epoch 119/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 120/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5457\n",
            "Epoch 121/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 122/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 123/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5457\n",
            "Epoch 124/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5458\n",
            "Epoch 125/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 126/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5457\n",
            "Epoch 127/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5457\n",
            "Epoch 128/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 129/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5450 - val_loss: 0.5456\n",
            "Epoch 130/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 131/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5457\n",
            "Epoch 132/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5457\n",
            "Epoch 133/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5459\n",
            "Epoch 134/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 135/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5458\n",
            "Epoch 136/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5457\n",
            "Epoch 137/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5457\n",
            "Epoch 138/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5457\n",
            "Epoch 139/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5457\n",
            "Epoch 140/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5457\n",
            "Epoch 141/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5460\n",
            "Epoch 142/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5458\n",
            "Epoch 143/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5457\n",
            "Epoch 144/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 145/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 146/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 147/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 148/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 149/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 150/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 151/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5457\n",
            "Epoch 152/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5456\n",
            "Epoch 153/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 154/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 155/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 156/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 157/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 158/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 159/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 160/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 161/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 162/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 163/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 164/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 165/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 166/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 167/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 168/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5456\n",
            "Epoch 169/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 170/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5455\n",
            "Epoch 171/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 172/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 173/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 174/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 175/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 176/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 177/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 178/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5456\n",
            "Epoch 179/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5456\n",
            "Epoch 180/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 181/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 182/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5453\n",
            "Epoch 183/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 184/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 185/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 186/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 187/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5455\n",
            "Epoch 188/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 189/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 190/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 191/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 192/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 193/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 194/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 195/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 196/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 197/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 198/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 199/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 200/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 201/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 202/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5454\n",
            "Epoch 203/250\n",
            "601/601 [==============================] - 2s 3ms/step - loss: 0.5448 - val_loss: 0.5455\n",
            "Epoch 204/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 205/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 206/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 207/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 208/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 209/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5449 - val_loss: 0.5454\n",
            "Epoch 210/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 211/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 212/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 213/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 214/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 215/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5454\n",
            "Epoch 216/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 217/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 218/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 219/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 220/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 221/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 222/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 223/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 224/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 225/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 226/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5454\n",
            "Epoch 227/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 228/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 229/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 230/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 231/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 232/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 233/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 234/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5444 - val_loss: 0.5455\n",
            "Epoch 235/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 236/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5454\n",
            "Epoch 237/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5454\n",
            "Epoch 238/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 239/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5455\n",
            "Epoch 240/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 241/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5454\n",
            "Epoch 242/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5445 - val_loss: 0.5455\n",
            "Epoch 243/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 244/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5445 - val_loss: 0.5454\n",
            "Epoch 245/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5455\n",
            "Epoch 246/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5446 - val_loss: 0.5454\n",
            "Epoch 247/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 248/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n",
            "Epoch 249/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5448 - val_loss: 0.5454\n",
            "Epoch 250/250\n",
            "601/601 [==============================] - 1s 2ms/step - loss: 0.5447 - val_loss: 0.5454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "wzICoLoa3kbX",
        "outputId": "ca291649-be19-4f66-f852-ea4712d5f7e1"
      },
      "source": [
        "plt.plot(history.history[\"loss\"])\r\n",
        "plt.plot(history.history[\"val_loss\"])\r\n",
        "plt.title(\"model loss\")\r\n",
        "plt.ylabel(\"loss\")\r\n",
        "plt.xlabel(\"epoch\")\r\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5syYS2-3oVi",
        "outputId": "c302ed69-e610-420e-8d52-08c032fe1b82"
      },
      "source": [
        "user_id = int(input()) \r\n",
        "#Hint : Available Customer IDs : 13047 , 12583 , 15291 , 14688 , 15311"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4xRt5yg3roB"
      },
      "source": [
        "top_purchased = get_recomended(user_id, 5)\r\n",
        "content= top_purchased .index.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEfYQDK531pn",
        "outputId": "bdce25cc-2afa-457f-a7bd-6fc2d1013a79"
      },
      "source": [
        "#Collaborative filtering\r\n",
        "ds_new = pd.read_csv('OnlineRetail.csv' ,encoding= 'unicode_escape',nrows=20000)\r\n",
        "\r\n",
        "purchased_items = df[df.CustomerID== user_id]\r\n",
        "not_purchased = data[~ds_new[\"StockCode\"].isin(purchased_items.StockCode.values)][\"StockCode\"]\r\n",
        "not_purchased = list(set(not_purchased).intersection(set(encode_stock.keys())))\r\n",
        "not_purchased = [[encode_stock.get(x)] for x in not_purchased]\r\n",
        "\r\n",
        "user_encoder = encode_user.get(user_id)\r\n",
        "user_items = np.hstack(([[user_encoder]] * len(not_purchased), not_purchased))\r\n",
        "\r\n",
        "quantities = model.predict(user_items).flatten()\r\n",
        "top_purchased = quantities.argsort()[-20:][::-1]\r\n",
        "\r\n",
        "recommended_stockid = [encoded_stockid.get(not_purchased[x][0]) for x in top_purchased]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZqIuDhE4Evv",
        "outputId": "f15b6131-ffe8-41d9-d893-e9563397a131"
      },
      "source": [
        "print(\"RECOMMENDED PRODUCTS | CUSTOMER ID :\",user_id, \"\\n\")\r\n",
        "\r\n",
        "recommending = random.sample((recommended_stockid), 10)\r\n",
        "recommended_products = ds_new[ds_new[\"StockCode\"].isin(recommending)]\r\n",
        "recomended_item_list = []\r\n",
        "for row in recommended_products.itertuples():\r\n",
        "    recomended_item_list .append(row.Description)\r\n",
        "unique_numbers = list(set(recomended_item_list ))\r\n",
        "print(*unique_numbers, sep = \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RECOMMENDED PRODUCTS | CUSTOMER ID : 12583 \n",
            "\n",
            "TOY TIDY PINK POLKADOT\n",
            "CD WALL TIDY RED FLOWERS\n",
            "VICTORIAN GLASS HANGING T-LIGHT\n",
            "WOODEN BOX ADVENT CALENDAR \n",
            "SET OF 36 PAISLEY FLOWER DOILIES\n",
            "VINTAGE RED KITCHEN CABINET\n",
            "COOKING SET RETROSPOT\n",
            "NAMASTE SWAGAT INCENSE\n",
            "SMALL POLKADOT CHOCOLATE GIFT BAG \n",
            "WORLD WAR 2 GLIDERS ASSTD DESIGNS\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}